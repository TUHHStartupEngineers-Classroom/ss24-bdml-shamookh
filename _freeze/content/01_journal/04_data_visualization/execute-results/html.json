{
  "hash": "58aacb3f5a8cb9a7f752d0b98774b4bb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Performance Measures\"\nauthor: \"Muhammad Shamookh\"\ndate: \"2024-06\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    df_print: paged\n    collapsed: false\n    number_sections: true\n    toc_depth: 3\n    code_folding: hide\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(h2o)\nlibrary(recipes)\nlibrary(rsample)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nproduct_backorders_tbl <- read_csv(\"./04_perf_meas_files/product_backorders.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Rows: 19053 Columns: 23\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#> dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(product_backorders_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 19,053\n#> Columns: 23\n#> $ sku               <dbl> 1113121, 1113268, 1113874, 1114222, 1114823, 1115453…\n#> $ national_inv      <dbl> 0, 0, 20, 0, 0, 55, -34, 4, 2, -7, 1, 2, 0, 0, 0, 0,…\n#> $ lead_time         <dbl> 8, 8, 2, 8, 12, 8, 8, 9, 8, 8, 8, 8, 12, 2, 12, 4, 2…\n#> $ in_transit_qty    <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n#> $ forecast_3_month  <dbl> 6, 2, 45, 9, 31, 216, 120, 43, 4, 56, 2, 5, 5, 54, 4…\n#> $ forecast_6_month  <dbl> 6, 3, 99, 14, 31, 360, 240, 67, 6, 96, 4, 9, 6, 72, …\n#> $ forecast_9_month  <dbl> 6, 4, 153, 21, 31, 492, 240, 115, 9, 112, 6, 13, 9, …\n#> $ sales_1_month     <dbl> 0, 1, 16, 5, 7, 30, 83, 5, 1, 13, 0, 1, 0, 0, 1, 0, …\n#> $ sales_3_month     <dbl> 4, 2, 42, 17, 15, 108, 122, 22, 5, 30, 2, 5, 4, 0, 3…\n#> $ sales_6_month     <dbl> 9, 3, 80, 36, 33, 275, 144, 40, 6, 56, 3, 8, 5, 0, 4…\n#> $ sales_9_month     <dbl> 12, 3, 111, 43, 47, 340, 165, 58, 9, 76, 4, 11, 6, 0…\n#> $ min_bank          <dbl> 0, 0, 10, 0, 2, 51, 33, 4, 2, 0, 0, 0, 3, 4, 0, 0, 0…\n#> $ potential_issue   <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ pieces_past_due   <dbl> 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ perf_6_month_avg  <dbl> 0.90, 0.96, 0.81, 0.96, 0.98, 0.00, 1.00, 0.69, 1.00…\n#> $ perf_12_month_avg <dbl> 0.89, 0.97, 0.88, 0.98, 0.98, 0.00, 0.97, 0.68, 0.95…\n#> $ local_bo_qty      <dbl> 0, 0, 0, 0, 0, 0, 34, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, …\n#> $ deck_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ oe_constraint     <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ ppap_risk         <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No…\n#> $ stop_auto_buy     <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n#> $ rev_stop          <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n#> $ went_on_backorder <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrecipe_obj <- recipe(went_on_backorder ~ ., data = product_backorders_tbl) %>%\n  step_zv(all_predictors()) %>%\n  step_mutate_at(potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, fn = as.factor) %>%\n  prep()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nsplit_obj <- initial_split(product_backorders_tbl, prop = 0.85)\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl <- testing(split_obj)\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nh2o.init()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         12 minutes 32 seconds \n#>     H2O cluster timezone:       Asia/Karachi \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.36.0.4 \n#>     H2O cluster version age:    2 years, 2 months and 24 days !!! \n#>     H2O cluster name:           H2O_started_from_R_Crown_Tech_pgz902 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.53 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.4.1 (2024-06-14 ucrt)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is too old (2 years, 2 months and 24 days)!\n#> Please download and install the latest version from http://h2o.ai/download/\n```\n\n\n:::\n\n```{.r .cell-code}\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 120,\n  nfolds            = 5 \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n#> 22:28:15.897: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 22:28:15.900: AutoML: XGBoost is not available; skipping it.\n#> 22:28:15.902: Step 'best_of_family_xgboost' not defined in provider 'StackedEnsemble': skipping it.\n#> 22:28:15.902: Step 'all_xgboost' not defined in provider 'StackedEnsemble': skipping it.\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(automl_models_h2o)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"S4\"\n```\n\n\n:::\n\n```{.r .cell-code}\nslotNames(automl_models_h2o)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#> [5] \"modeling_steps\" \"training_info\"\n```\n\n\n:::\n\n```{.r .cell-code}\nautoml_models_h2o@leaderboard\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                                                  model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_3_20240623_222815 0.9528473 0.1742689\n#> 2    StackedEnsemble_AllModels_1_AutoML_3_20240623_222815 0.9527280 0.1757474\n#> 3 StackedEnsemble_BestOfFamily_3_AutoML_3_20240623_222815 0.9508354 0.1771444\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_3_20240623_222815 0.9501388 0.1792685\n#> 5                          GBM_4_AutoML_3_20240623_222815 0.9495915 0.1818160\n#> 6 StackedEnsemble_BestOfFamily_1_AutoML_3_20240623_222815 0.9494060 0.1799105\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7566698            0.1495575 0.2299987 0.05289938\n#> 2 0.7531899            0.1363528 0.2308837 0.05330729\n#> 3 0.7442111            0.1444714 0.2314254 0.05355771\n#> 4 0.7390554            0.1383872 0.2325584 0.05408340\n#> 5 0.7333920            0.1361340 0.2343808 0.05493437\n#> 6 0.7430157            0.1398037 0.2340836 0.05479513\n#> \n#> [14 rows x 7 columns]\n```\n\n\n:::\n\n```{.r .cell-code}\nautoml_models_h2o@leader\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_2_AutoML_3_20240623_222815 \n#> Number of Base Models: 9\n#> \n#> Base Models (count by algorithm type):\n#> \n#> deeplearning          drf          gbm          glm \n#>            1            2            5            1 \n#> \n#> Metalearner:\n#> \n#> Metalearner algorithm: glm\n#> Metalearner cross-validation fold assignment:\n#>   Fold assignment scheme: AUTO\n#>   Number of folds: 5\n#>   Fold column: NULL\n#> Metalearner hyperparameters: \n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.0262093\n#> RMSE:  0.1618929\n#> LogLoss:  0.09774234\n#> Mean Per-Class Error:  0.08191296\n#> AUC:  0.9888977\n#> AUCPR:  0.9397221\n#> Gini:  0.9777953\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error       Rate\n#> No     8618  140 0.015985  =140/8758\n#> Yes     178 1026 0.147841  =178/1204\n#> Totals 8796 1166 0.031921  =318/9962\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.442038    0.865823 166\n#> 2                       max f2  0.263113    0.891497 221\n#> 3                 max f0point5  0.603386    0.899763 125\n#> 4                 max accuracy  0.498424    0.968480 152\n#> 5                max precision  0.992235    1.000000   0\n#> 6                   max recall  0.026407    1.000000 343\n#> 7              max specificity  0.992235    1.000000   0\n#> 8             max absolute_mcc  0.442038    0.847853 166\n#> 9   max min_per_class_accuracy  0.212188    0.943522 239\n#> 10 max mean_per_class_accuracy  0.263113    0.945996 221\n#> 11                     max tns  0.992235 8758.000000   0\n#> 12                     max fns  0.992235 1202.000000   0\n#> 13                     max fps  0.000150 8758.000000 399\n#> 14                     max tps  0.026407 1204.000000 343\n#> 15                     max tnr  0.992235    1.000000   0\n#> 16                     max fnr  0.992235    0.998339   0\n#> 17                     max fpr  0.000150    1.000000 399\n#> 18                     max tpr  0.026407    1.000000 343\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.04479636\n#> RMSE:  0.2116515\n#> LogLoss:  0.1501092\n#> Mean Per-Class Error:  0.1401232\n#> AUC:  0.960825\n#> AUCPR:  0.775796\n#> Gini:  0.9216501\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2042  87 0.040864   =87/2129\n#> Yes      62 197 0.239382    =62/259\n#> Totals 2104 284 0.062395  =149/2388\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.386767    0.725599 161\n#> 2                       max f2  0.164770    0.794679 234\n#> 3                 max f0point5  0.601508    0.747801 107\n#> 4                 max accuracy  0.601508    0.939698 107\n#> 5                max precision  0.972451    1.000000   0\n#> 6                   max recall  0.012311    1.000000 363\n#> 7              max specificity  0.972451    1.000000   0\n#> 8             max absolute_mcc  0.386767    0.691416 161\n#> 9   max min_per_class_accuracy  0.158203    0.902771 238\n#> 10 max mean_per_class_accuracy  0.164770    0.903541 234\n#> 11                     max tns  0.972451 2129.000000   0\n#> 12                     max fns  0.972451  258.000000   0\n#> 13                     max fps  0.000056 2129.000000 399\n#> 14                     max tps  0.012311  259.000000 363\n#> 15                     max tnr  0.972451    1.000000   0\n#> 16                     max fnr  0.972451    0.996139   0\n#> 17                     max fpr  0.000056    1.000000 399\n#> 18                     max tpr  0.012311    1.000000 363\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.05091892\n#> RMSE:  0.2256522\n#> LogLoss:  0.1717777\n#> Mean Per-Class Error:  0.1526662\n#> AUC:  0.9502735\n#> AUCPR:  0.7493797\n#> Gini:  0.900547\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     11543  610 0.050193   =610/12153\n#> Yes      422 1232 0.255139    =422/1654\n#> Totals 11965 1842 0.074745  =1032/13807\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.331908     0.704805 208\n#> 2                       max f2  0.111599     0.773585 292\n#> 3                 max f0point5  0.520605     0.733964 146\n#> 4                 max accuracy  0.520605     0.932498 146\n#> 5                max precision  0.989240     1.000000   0\n#> 6                   max recall  0.002941     1.000000 392\n#> 7              max specificity  0.989240     1.000000   0\n#> 8             max absolute_mcc  0.401400     0.664022 184\n#> 9   max min_per_class_accuracy  0.117577     0.883918 289\n#> 10 max mean_per_class_accuracy  0.111599     0.886617 292\n#> 11                     max tns  0.989240 12153.000000   0\n#> 12                     max fns  0.989240  1651.000000   0\n#> 13                     max fps  0.000088 12153.000000 399\n#> 14                     max tps  0.002941  1654.000000 392\n#> 15                     max tnr  0.989240     1.000000   0\n#> 16                     max fnr  0.989240     0.998186   0\n#> 17                     max fpr  0.000088     1.000000 399\n#> 18                     max tpr  0.002941     1.000000 392\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n\n\n:::\n\n```{.r .cell-code}\ntypeof(automl_models_h2o@leader)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"S4\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- h2o.predict(automl_models_h2o@leader, newdata = as.h2o(test_tbl))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\npredictions_tbl <- \n  predictions %>% \n    as_tibble()\n#h2o.saveModel(automl_models_h2o@leader, path = \"./04_perf_meas_files/\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions_tbl %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 2,858\n#> Columns: 3\n#> $ predict <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, …\n#> $ No      <dbl> 0.35099977, 0.49446699, 0.10396982, 0.15413307, 0.06619593, 0.…\n#> $ Yes     <dbl> 0.64900023, 0.50553301, 0.89603018, 0.84586693, 0.93380407, 0.…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoml_models_h2o@leaderboard %>% \n              as_tibble() %>% \n              select(-c(mean_per_class_error, rmse, mse))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"model_id\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"auc\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"logloss\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"aucpr\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"StackedEnsemble_AllModels_2_AutoML_3_20240623_222815\",\"2\":\"0.9528473\",\"3\":\"0.1742689\",\"4\":\"0.7566698\"},{\"1\":\"StackedEnsemble_AllModels_1_AutoML_3_20240623_222815\",\"2\":\"0.9527280\",\"3\":\"0.1757474\",\"4\":\"0.7531899\"},{\"1\":\"StackedEnsemble_BestOfFamily_3_AutoML_3_20240623_222815\",\"2\":\"0.9508354\",\"3\":\"0.1771444\",\"4\":\"0.7442111\"},{\"1\":\"StackedEnsemble_BestOfFamily_2_AutoML_3_20240623_222815\",\"2\":\"0.9501388\",\"3\":\"0.1792685\",\"4\":\"0.7390554\"},{\"1\":\"GBM_4_AutoML_3_20240623_222815\",\"2\":\"0.9495915\",\"3\":\"0.1818160\",\"4\":\"0.7333920\"},{\"1\":\"StackedEnsemble_BestOfFamily_1_AutoML_3_20240623_222815\",\"2\":\"0.9494060\",\"3\":\"0.1799105\",\"4\":\"0.7430157\"},{\"1\":\"GBM_1_AutoML_3_20240623_222815\",\"2\":\"0.9494026\",\"3\":\"0.1805603\",\"4\":\"0.7429160\"},{\"1\":\"GBM_3_AutoML_3_20240623_222815\",\"2\":\"0.9488242\",\"3\":\"0.1827452\",\"4\":\"0.7433104\"},{\"1\":\"GBM_2_AutoML_3_20240623_222815\",\"2\":\"0.9472104\",\"3\":\"0.1853520\",\"4\":\"0.7352002\"},{\"1\":\"GBM_5_AutoML_3_20240623_222815\",\"2\":\"0.9376154\",\"3\":\"0.2059936\",\"4\":\"0.6958716\"},{\"1\":\"XRT_1_AutoML_3_20240623_222815\",\"2\":\"0.9310280\",\"3\":\"0.2218990\",\"4\":\"0.7052454\"},{\"1\":\"DRF_1_AutoML_3_20240623_222815\",\"2\":\"0.9269976\",\"3\":\"0.2320392\",\"4\":\"0.6912053\"},{\"1\":\"DeepLearning_1_AutoML_3_20240623_222815\",\"2\":\"0.8094593\",\"3\":\"0.3103805\",\"4\":\"0.3984400\"},{\"1\":\"GLM_1_AutoML_3_20240623_222815\",\"2\":\"0.7678139\",\"3\":\"0.3274070\",\"4\":\"0.3028988\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"), \n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n\n    # Setup inputs\n    # adjust input so that all formats are working\n    order_by <- tolower(order_by[[1]])\n\n    leaderboard_tbl <- h2o_leaderboard %>%\n        as_tibble() %>%\n        select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n        mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n        rownames_to_column(var = \"rowname\") %>%\n        mutate(model_id = paste0(rowname, \". \", model_id) %>% as.factor())\n\n    # Transformation\n    if (order_by == \"auc\") {\n\n        data_transformed_tbl <- leaderboard_tbl %>%\n            slice(1:n_max) %>%\n            mutate(\n                model_id   = as_factor(model_id) %>% reorder(auc),\n                model_type = as.factor(model_type)\n            ) %>%\n                pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       )\n\n    } else if (order_by == \"logloss\") {\n\n        data_transformed_tbl <- leaderboard_tbl %>%\n            slice(1:n_max) %>%\n            mutate(\n                model_id   = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),\n                model_type = as.factor(model_type)\n            ) %>%\n            pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       )\n\n    } else {\n        # If nothing is supplied\n        stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n    }\n\n    # Visualization\n    g <- data_transformed_tbl %>%\n        ggplot(aes(value, model_id, color = model_type)) +\n        geom_point(size = size) +\n        facet_wrap(~ key, scales = \"free_x\") +\n        labs(title = \"Leaderboard Metrics\",\n             subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n             y = \"Model Postion, Model ID\", x = \"\")\n\n    if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), \n                                             hjust = \"inward\"))\n\n    return(g)\n\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#automl_models_h2o@leaderboard %>% plot_h2o_leaderboard()\n\n\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-12-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nFor some reason I can not build the page because the program have some problem with an h2o function. So I saved the results and loaded them. The used code is commented out.\n\n::: {.cell}\n\n```{.r .cell-code}\n#h2o.init()\n#deeplearning_h2o <- \n#    h2o.loadModel(\"./04_Modeling/h20_models/DeepLearning_1_AutoML_3_20220614_234925\")\n#deeplearning_h2o@allparameters\n\n# Deeplearning_grid_01 <- h2o.grid(\n# \n#     # See help page for available algos\n#     algorithm = \"deeplearning\",\n#     \n#     # I just use the same as the object\n#     grid_id = \"Deaplearning_grid_01\",\n#     \n#     # The following is for ?h2o.deeplearning()\n#     # predictor and response variables\n#     x = x,\n#     y = y,\n#     \n#     # training and validation frame and crossfold validation\n#     training_frame   = train_h2o,\n#     validation_frame = valid_h2o,\n#     nfolds = 5,\n#     \n#     # Hyperparamters: Use deeplearning_h2o@allparameters to see all\n#     hyper_params = list(\n#         # Use some combinations (the first one was the original)\n#         hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n#         epochs = c(10, 50, 100)\n#     )\n# )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# <- h2o.getModel(\"Deaplearning_grid_01_model_3\")\n#Deeplearning_grid_01_model_3 %>%h2o.saveModel(path = \"04_Modeling/Deaplearning_grid_01_model_3\")\n#Deeplearning_grid_01_model_3 <- h2o.loadModel(\"04_Modeling/Deaplearning_grid_01_model_3/Deaplearning_grid_01_model_3\")\n# performance_h2o <- h2o.performance(Deeplearning_grid_01_model_3, newdata = as.h2o(test_tbl))\n# \n# performance_tbl <- performance_h2o %>%\n#     h2o.metric() %>%\n#     as.tibble()\n# \n# theme_new <- theme(\n#       legend.position  = \"bottom\",\n#       panel.background = element_rect(fill   = \"transparent\"),\n#       panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n#       panel.grid.major = element_line(color = \"grey\", size = 0.333)\n#       ) \n# saveRDS(performance_tbl, file = \"performance_tbl.rds\")\n\nperformance_tbl <- readRDS(\"performance_tbl.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#performance_tbl %>%\n#    filter(f1 == max(f1))\n#\n#performance_tbl %>%\n#    ggplot(aes(x = threshold)) +\n#    geom_line(aes(y = precision), color = \"blue\", size = 1) +\n#    geom_line(aes(y = recall), color = \"red\", size = 1) +\n#    \n#    # Insert line where precision and recall are harmonically optimized\n#    geom_vline(xintercept = #h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n#    labs(title = \"Precision vs Recall\", y = \"value\") +\n#    theme_new\n#\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-15-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#p1 <- performance_tbl %>%\n#  ggplot(aes(fpr, tpr)) +\n#    geom_line(size = 1) +\n#    \n#    # just for demonstration purposes\n#    geom_abline(color = \"red\", linetype = \"dotted\") +\n#    \n#    theme_new +\n#    theme(\n#      legend.direction = \"vertical\",\n#      ) +\n#    labs(\n#        title = \"ROC Plot\"\n#        #subtitle = \"Performance of 3 Top Performing Models\"\n#    )\n#p1\n#\n\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-16-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#p2 <- performance_tbl %>%\n#  ggplot(aes(recall, precision)) +\n#    geom_line(size = 1) +\n#    theme_new + \n#    theme(\n#      legend.direction = \"vertical\",\n#      ) +\n#    labs(\n#        title = \"Precision vs Recall Plot\"\n#        #subtitle = \"Performance of 3 Top Performing Models\"\n#    )\n#p2\n\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-17-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nranked_predictions_tbl <- predictions_tbl %>%\n    bind_cols(test_tbl) %>%\n    select(predict:Yes, went_on_backorder) %>%\n    # Sorting from highest to lowest class probability\n    arrange(desc(Yes))\n\ncalculated_gain_lift_tbl <- ranked_predictions_tbl %>%\n    mutate(ntile = ntile(Yes, n = 10)) %>%\n    group_by(ntile) %>%\n    summarise(\n        cases = n(),\n        responses = sum(went_on_backorder == \"Yes\")\n    ) %>%\n    arrange(desc(ntile)) %>%\n    \n    # Add group numbers (opposite of ntile)\n    mutate(group = row_number()) %>%\n    select(group, cases, responses) %>%\n    \n    # Calculations\n    mutate(\n        cumulative_responses = cumsum(responses),\n        pct_responses        = responses / sum(responses),\n        gain                 = cumsum(pct_responses),\n        cumulative_pct_cases = cumsum(cases) / sum(cases),\n        lift                 = gain / cumulative_pct_cases,\n        gain_baseline        = cumulative_pct_cases,\n        lift_baseline        = gain_baseline / cumulative_pct_cases\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#gain_lift_tbl <- performance_h2o %>%\n#    h2o.gainsLift() %>%\n#    as.tibble()\n#\n#gain_transformed_tbl <- gain_lift_tbl %>% \n#    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n#    select(-contains(\"lift\")) %>%\n#    mutate(baseline = cumulative_data_fraction) %>%\n#    rename(gain     = cumulative_capture_rate) %>%\n#    # prepare the data for the plotting (for the color and group aesthetics)\n#    pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n#\n#p3 <- gain_transformed_tbl %>%\n#    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n#    geom_line(size = 1.5) +\n#    labs(\n#        title = \"Gain Chart\",\n#        x = \"Cumulative Data Fraction\",\n#        y = \"Gain\"\n#    ) +\n#    theme_new\n#p3\n\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-19-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#lift_transformed_tbl <- gain_lift_tbl %>% \n#    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n#    select(-contains(\"capture\")) %>%\n#    mutate(baseline = 1) %>%\n#    rename(lift = cumulative_lift) %>%\n#    pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n#\n#p4 <- lift_transformed_tbl %>%\n#    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n#    geom_line(size = 1.5) +\n#    labs(\n#        title = \"Lift Chart\",\n#        x = \"Cumulative Data Fraction\",\n#        y = \"Lift\"\n#    ) +\n#    theme_new\n#p4\n\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-20-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n#> Attaching package: 'cowplot'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> The following object is masked from 'package:lubridate':\n#> \n#>     stamp\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(glue)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine using cowplot\n   # \n#    # cowplot::get_legend extracts a legend from a ggplot object\n  #  p_legend <- get_legend(p1)\n#    # Remove legend from p1\n #   p1 <- p1 + theme(legend.position = \"none\")\n    \n    # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n#    p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n #   p\nknitr::include_graphics(\"./04_perf_meas_files/figure-html/unnamed-chunk-22-1.png\")\n```\n\n::: {.cell-output-display}\n![](./04_perf_meas_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}